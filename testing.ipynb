{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTConfig(block_size=1024, vocab_size=100288, n_layer=16, n_head=16, n_embd=1024, attn_dropout=0.0, pad_token_id=100278, batch_size=4, epochs=2, clip_grad_norm_val=1.0, training_backend='nccl', learning_rate=0.0003, eta_min=3e-06, weight_decay=0.0001, eps=1e-08, betas=(0.9, 0.97), base_theta=10000.0, scale_factor=1.0, gradient_accumulation_steps=32, warmup_steps=None, warmup_steps_ratio=0.15, total_steps=None, steps_per_epoch=None, dtype=torch.bfloat16, fused_optimizer=True, do_init_params=True, rng_seed=42, rng_device=device(type='cpu'), model_device=device(type='cpu'), rng_generator=<torch._C.Generator object at 0x000001E4B7EA1230>)\n"
     ]
    }
   ],
   "source": [
    "gin.parse_config_file(\"config/gpt2-small.gin\")\n",
    "config = GPTConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(config)\n",
    "# model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pn, p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(pn, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 81, with 371,130,368 parameters\n",
      "num non-decayed parameter tensors: 147, with 380,864 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 371.51 million\n"
     ]
    }
   ],
   "source": [
    "# Print the number of parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "num_parameters_in_millions = num_parameters / 1e6\n",
    "print(f\"Number of parameters: {num_parameters_in_millions:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# gpt4_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"<|startoftext|>\": gpt4_tokenizer.n_vocab,\n",
    "    \"<|PAD|>\": gpt4_tokenizer.n_vocab + 1,\n",
    "    \"<|TASK|>\": gpt4_tokenizer.n_vocab + 2,\n",
    "    \"<|MASK|>\": gpt4_tokenizer.n_vocab + 3,\n",
    "    \"<|CLS|>\": gpt4_tokenizer.n_vocab + 4,\n",
    "    \"<|USER|>\": gpt4_tokenizer.n_vocab + 5,\n",
    "    \"<|ASSISTANT|>\": gpt4_tokenizer.n_vocab + 6,\n",
    "    \"<|SYSTEM|>\": gpt4_tokenizer.n_vocab + 7,\n",
    "    \"<|im_start|>\": gpt4_tokenizer.n_vocab + 8,\n",
    "    \"<|im_end|>\": gpt4_tokenizer.n_vocab + 9,\n",
    "    \"<|im_sep|>\": gpt4_tokenizer.n_vocab + 10,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer = tiktoken.Encoding(\n",
    "    name=\"custom\",\n",
    "    pat_str=gpt4_tokenizer._pat_str,\n",
    "    mergeable_ranks=gpt4_tokenizer._mergeable_ranks,\n",
    "    special_tokens={**gpt4_tokenizer._special_tokens, **special_tokens},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 100257,\n",
       " '<|fim_prefix|>': 100258,\n",
       " '<|fim_middle|>': 100259,\n",
       " '<|fim_suffix|>': 100260,\n",
       " '<|endofprompt|>': 100276,\n",
       " '<|startoftext|>': 100277,\n",
       " '<|PAD|>': 100278,\n",
       " '<|TASK|>': 100279,\n",
       " '<|MASK|>': 100280,\n",
       " '<|CLS|>': 100281,\n",
       " '<|USER|>': 100282,\n",
       " '<|ASSISTANT|>': 100283,\n",
       " '<|SYSTEM|>': 100284,\n",
       " '<|im_start|>': 100285,\n",
       " '<|im_end|>': 100286,\n",
       " '<|im_sep|>': 100287}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer._special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a = nn.Embedding(2, 4, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = custom_tokenizer._mergeable_ranks\n",
    "# special_tokens = custom_tokenizer._special_tokens\n",
    "\n",
    "# import json\n",
    "# full_vocab = {**vocab, **special_tokens}\n",
    "# with open(\"vocab.json\", \"w\") as f:\n",
    "#     json.dump(full_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample input\n",
    "text = torch.randint(0, 100288, (1, 1024))\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(text)\n",
    "# output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100288, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# def generate(model,\n",
    "#              tokenizer,\n",
    "#              prompt: str,\n",
    "#              n_tokens_to_gen: int = 200,\n",
    "#              sample: bool = True,\n",
    "#              top_k: int = 40):\n",
    "#     model.eval()\n",
    "\n",
    "#     input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "#     for token_n in range(n_tokens_to_gen):\n",
    "#         with torch.no_grad():\n",
    "#             indices_to_input = input_ids\n",
    "#             next_token_logits, loss = model(indices_to_input)\n",
    "#             next_token_logits = next_token_logits[:, -1]\n",
    "\n",
    "#         probs = F.softmax(next_token_logits, dim=-1)\n",
    "#         (batch, vocab_size) = probs.shape\n",
    "\n",
    "#         if top_k is not None:\n",
    "#             (values, indices) = torch.topk(probs, k=top_k)\n",
    "#             probs[probs < values[:, -1, None]] = 0\n",
    "#             probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "#         if sample:\n",
    "#             next_indices = torch.multinomial(probs, num_samples=1)\n",
    "#         else:\n",
    "#             next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "\n",
    "#         input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "#     output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "\n",
    "#     return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
