{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTConfig(block_size=1024, vocab_size=100288, n_layer=16, n_head=16, n_embd=1024, attn_dropout=0.0, pad_token_id=100278, batch_size=4, epochs=2, clip_grad_norm_val=1.0, training_backend='nccl', learning_rate=0.0003, eta_min=3e-06, weight_decay=0.0001, eps=1e-08, betas=(0.9, 0.97), base_theta=10000.0, scale_factor=1.0, gradient_accumulation_steps=8, warmup_steps=None, warmup_steps_ratio=0.15, total_steps=None, steps_per_epoch=None, dtype=torch.bfloat16, fused_optimizer=True, do_init_params=True, rng_seed=42, rng_device=device(type='cpu'), model_device=device(type='cpu'), rng_generator=<torch._C.Generator object at 0x0000019880402470>)\n"
     ]
    }
   ],
   "source": [
    "gin.parse_config_file(\"config/gpt2-small.gin\")\n",
    "config = GPTConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(config)\n",
    "# model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([100288, 1024])\n",
      "transformer.h.0.ln_1.weight torch.Size([1024])\n",
      "transformer.h.0.ln_1.bias torch.Size([1024])\n",
      "transformer.h.0.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.0.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.0.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.0.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.0.ln_2.weight torch.Size([1024])\n",
      "transformer.h.0.ln_2.bias torch.Size([1024])\n",
      "transformer.h.0.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.0.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.0.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.0.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.0.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.0.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.1.ln_1.weight torch.Size([1024])\n",
      "transformer.h.1.ln_1.bias torch.Size([1024])\n",
      "transformer.h.1.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.1.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.1.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.1.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.1.ln_2.weight torch.Size([1024])\n",
      "transformer.h.1.ln_2.bias torch.Size([1024])\n",
      "transformer.h.1.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.1.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.1.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.1.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.1.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.1.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.2.ln_1.weight torch.Size([1024])\n",
      "transformer.h.2.ln_1.bias torch.Size([1024])\n",
      "transformer.h.2.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.2.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.2.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.2.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.2.ln_2.weight torch.Size([1024])\n",
      "transformer.h.2.ln_2.bias torch.Size([1024])\n",
      "transformer.h.2.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.2.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.2.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.2.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.2.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.2.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.3.ln_1.weight torch.Size([1024])\n",
      "transformer.h.3.ln_1.bias torch.Size([1024])\n",
      "transformer.h.3.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.3.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.3.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.3.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.3.ln_2.weight torch.Size([1024])\n",
      "transformer.h.3.ln_2.bias torch.Size([1024])\n",
      "transformer.h.3.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.3.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.3.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.3.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.3.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.3.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.4.ln_1.weight torch.Size([1024])\n",
      "transformer.h.4.ln_1.bias torch.Size([1024])\n",
      "transformer.h.4.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.4.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.4.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.4.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.4.ln_2.weight torch.Size([1024])\n",
      "transformer.h.4.ln_2.bias torch.Size([1024])\n",
      "transformer.h.4.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.4.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.4.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.4.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.4.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.4.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.5.ln_1.weight torch.Size([1024])\n",
      "transformer.h.5.ln_1.bias torch.Size([1024])\n",
      "transformer.h.5.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.5.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.5.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.5.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.5.ln_2.weight torch.Size([1024])\n",
      "transformer.h.5.ln_2.bias torch.Size([1024])\n",
      "transformer.h.5.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.5.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.5.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.5.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.5.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.5.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.6.ln_1.weight torch.Size([1024])\n",
      "transformer.h.6.ln_1.bias torch.Size([1024])\n",
      "transformer.h.6.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.6.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.6.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.6.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.6.ln_2.weight torch.Size([1024])\n",
      "transformer.h.6.ln_2.bias torch.Size([1024])\n",
      "transformer.h.6.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.6.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.6.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.6.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.6.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.6.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.7.ln_1.weight torch.Size([1024])\n",
      "transformer.h.7.ln_1.bias torch.Size([1024])\n",
      "transformer.h.7.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.7.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.7.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.7.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.7.ln_2.weight torch.Size([1024])\n",
      "transformer.h.7.ln_2.bias torch.Size([1024])\n",
      "transformer.h.7.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.7.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.7.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.7.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.7.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.7.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.8.ln_1.weight torch.Size([1024])\n",
      "transformer.h.8.ln_1.bias torch.Size([1024])\n",
      "transformer.h.8.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.8.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.8.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.8.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.8.ln_2.weight torch.Size([1024])\n",
      "transformer.h.8.ln_2.bias torch.Size([1024])\n",
      "transformer.h.8.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.8.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.8.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.8.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.8.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.8.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.9.ln_1.weight torch.Size([1024])\n",
      "transformer.h.9.ln_1.bias torch.Size([1024])\n",
      "transformer.h.9.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.9.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.9.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.9.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.9.ln_2.weight torch.Size([1024])\n",
      "transformer.h.9.ln_2.bias torch.Size([1024])\n",
      "transformer.h.9.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.9.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.9.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.9.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.9.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.9.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.10.ln_1.weight torch.Size([1024])\n",
      "transformer.h.10.ln_1.bias torch.Size([1024])\n",
      "transformer.h.10.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.10.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.10.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.10.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.10.ln_2.weight torch.Size([1024])\n",
      "transformer.h.10.ln_2.bias torch.Size([1024])\n",
      "transformer.h.10.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.10.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.10.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.10.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.10.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.10.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.11.ln_1.weight torch.Size([1024])\n",
      "transformer.h.11.ln_1.bias torch.Size([1024])\n",
      "transformer.h.11.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.11.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.11.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.11.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.11.ln_2.weight torch.Size([1024])\n",
      "transformer.h.11.ln_2.bias torch.Size([1024])\n",
      "transformer.h.11.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.11.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.11.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.11.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.11.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.11.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.12.ln_1.weight torch.Size([1024])\n",
      "transformer.h.12.ln_1.bias torch.Size([1024])\n",
      "transformer.h.12.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.12.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.12.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.12.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.12.ln_2.weight torch.Size([1024])\n",
      "transformer.h.12.ln_2.bias torch.Size([1024])\n",
      "transformer.h.12.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.12.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.12.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.12.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.12.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.12.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.13.ln_1.weight torch.Size([1024])\n",
      "transformer.h.13.ln_1.bias torch.Size([1024])\n",
      "transformer.h.13.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.13.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.13.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.13.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.13.ln_2.weight torch.Size([1024])\n",
      "transformer.h.13.ln_2.bias torch.Size([1024])\n",
      "transformer.h.13.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.13.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.13.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.13.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.13.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.13.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.14.ln_1.weight torch.Size([1024])\n",
      "transformer.h.14.ln_1.bias torch.Size([1024])\n",
      "transformer.h.14.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.14.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.14.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.14.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.14.ln_2.weight torch.Size([1024])\n",
      "transformer.h.14.ln_2.bias torch.Size([1024])\n",
      "transformer.h.14.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.14.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.14.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.14.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.14.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.14.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.h.15.ln_1.weight torch.Size([1024])\n",
      "transformer.h.15.ln_1.bias torch.Size([1024])\n",
      "transformer.h.15.attn.qkv_proj.weight torch.Size([3072, 1024])\n",
      "transformer.h.15.attn.qkv_proj.bias torch.Size([3072])\n",
      "transformer.h.15.attn.o_proj.weight torch.Size([1024, 1024])\n",
      "transformer.h.15.attn.o_proj.bias torch.Size([1024])\n",
      "transformer.h.15.ln_2.weight torch.Size([1024])\n",
      "transformer.h.15.ln_2.bias torch.Size([1024])\n",
      "transformer.h.15.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.15.mlp.gate_proj.bias torch.Size([4096])\n",
      "transformer.h.15.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "transformer.h.15.mlp.up_proj.bias torch.Size([4096])\n",
      "transformer.h.15.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "transformer.h.15.mlp.down_proj.bias torch.Size([1024])\n",
      "transformer.ln_f.weight torch.Size([1024])\n",
      "transformer.ln_f.bias torch.Size([1024])\n",
      "lm_head.bias torch.Size([100288])\n"
     ]
    }
   ],
   "source": [
    "for pn, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(pn, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 81, with 371,130,368 parameters\n",
      "num non-decayed parameter tensors: 147, with 380,864 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371.511232"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of parameters in terms of billions\n",
    "num_params = sum([param.nelement() for param in model.parameters()])\n",
    "num_params / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# gpt4_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"<|startoftext|>\": gpt4_tokenizer.n_vocab,\n",
    "    \"<|PAD|>\": gpt4_tokenizer.n_vocab + 1,\n",
    "    \"<|TASK|>\": gpt4_tokenizer.n_vocab + 2,\n",
    "    \"<|MASK|>\": gpt4_tokenizer.n_vocab + 3,\n",
    "    \"<|CLS|>\": gpt4_tokenizer.n_vocab + 4,\n",
    "    \"<|USER|>\": gpt4_tokenizer.n_vocab + 5,\n",
    "    \"<|ASSISTANT|>\": gpt4_tokenizer.n_vocab + 6,\n",
    "    \"<|SYSTEM|>\": gpt4_tokenizer.n_vocab + 7,\n",
    "    \"<|im_start|>\": gpt4_tokenizer.n_vocab + 8,\n",
    "    \"<|im_end|>\": gpt4_tokenizer.n_vocab + 9,\n",
    "    \"<|im_sep|>\": gpt4_tokenizer.n_vocab + 10,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer = tiktoken.Encoding(\n",
    "    name=\"custom\",\n",
    "    pat_str=gpt4_tokenizer._pat_str,\n",
    "    mergeable_ranks=gpt4_tokenizer._mergeable_ranks,\n",
    "    special_tokens={**gpt4_tokenizer._special_tokens, **special_tokens},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 100257,\n",
       " '<|fim_prefix|>': 100258,\n",
       " '<|fim_middle|>': 100259,\n",
       " '<|fim_suffix|>': 100260,\n",
       " '<|endofprompt|>': 100276,\n",
       " '<|startoftext|>': 100277,\n",
       " '<|PAD|>': 100278,\n",
       " '<|TASK|>': 100279,\n",
       " '<|MASK|>': 100280,\n",
       " '<|CLS|>': 100281,\n",
       " '<|USER|>': 100282,\n",
       " '<|ASSISTANT|>': 100283,\n",
       " '<|SYSTEM|>': 100284,\n",
       " '<|im_start|>': 100285,\n",
       " '<|im_end|>': 100286,\n",
       " '<|im_sep|>': 100287}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer._special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a = nn.Embedding(2, 4, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'1'\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m full_vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvocab, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mspecial_tokens}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sathi\\miniconda3\\envs\\dl\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "vocab = custom_tokenizer._mergeable_ranks\n",
    "special_tokens = custom_tokenizer._special_tokens\n",
    "\n",
    "import json\n",
    "full_vocab = {**vocab, **special_tokens}\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(full_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample input\n",
    "text = torch.randint(0, 100288, (1, 1024))\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(text)\n",
    "# output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100288, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# def generate(model,\n",
    "#              tokenizer,\n",
    "#              prompt: str,\n",
    "#              n_tokens_to_gen: int = 200,\n",
    "#              sample: bool = True,\n",
    "#              top_k: int = 40):\n",
    "#     model.eval()\n",
    "\n",
    "#     input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "#     for token_n in range(n_tokens_to_gen):\n",
    "#         with torch.no_grad():\n",
    "#             indices_to_input = input_ids\n",
    "#             next_token_logits, loss = model(indices_to_input)\n",
    "#             next_token_logits = next_token_logits[:, -1]\n",
    "\n",
    "#         probs = F.softmax(next_token_logits, dim=-1)\n",
    "#         (batch, vocab_size) = probs.shape\n",
    "\n",
    "#         if top_k is not None:\n",
    "#             (values, indices) = torch.topk(probs, k=top_k)\n",
    "#             probs[probs < values[:, -1, None]] = 0\n",
    "#             probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "#         if sample:\n",
    "#             next_indices = torch.multinomial(probs, num_samples=1)\n",
    "#         else:\n",
    "#             next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "\n",
    "#         input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "#     output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "\n",
    "#     return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
