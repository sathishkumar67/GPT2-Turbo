{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTConfig(block_size=1024, vocab_size=100288, n_layer=16, n_head=16, n_embd=1024, batch_size=4, epochs=2, device='cuda', clip_grad_norm_val=1.0, training_backend='nccl', learning_rate=0.0003, weight_decay=0.0001, eps=1e-08, betas=(0.9, 0.97), base_theta=10000.0, scale_factor=1.0, dtype=torch.bfloat16, fused_optimizer=True)\n"
     ]
    }
   ],
   "source": [
    "gin.parse_config_file(\"config/gpt2-small.gin\")\n",
    "config = GPTConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(100288, 1024)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=100288, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(config)\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371.16416"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of parameters in terms of billions\n",
    "num_params = sum([param.nelement() for param in model.parameters()])\n",
    "num_params / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# gpt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# gpt4_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample input\n",
    "text = torch.randint(0, 100288, (1, 1024))\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(text)\n",
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# def generate(model,\n",
    "#              tokenizer,\n",
    "#              prompt: str,\n",
    "#              n_tokens_to_gen: int = 200,\n",
    "#              sample: bool = True,\n",
    "#              top_k: int = 40):\n",
    "#     model.eval()\n",
    "\n",
    "#     input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "#     for token_n in range(n_tokens_to_gen):\n",
    "#         with torch.no_grad():\n",
    "#             indices_to_input = input_ids\n",
    "#             next_token_logits, loss = model(indices_to_input)\n",
    "#             next_token_logits = next_token_logits[:, -1]\n",
    "\n",
    "#         probs = F.softmax(next_token_logits, dim=-1)\n",
    "#         (batch, vocab_size) = probs.shape\n",
    "\n",
    "#         if top_k is not None:\n",
    "#             (values, indices) = torch.topk(probs, k=top_k)\n",
    "#             probs[probs < values[:, -1, None]] = 0\n",
    "#             probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "#         if sample:\n",
    "#             next_indices = torch.multinomial(probs, num_samples=1)\n",
    "#         else:\n",
    "#             next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "\n",
    "#         input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "#     output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "\n",
    "#     return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
