# for the first phase of pretraining
    # init model using init method
    # setting grad accum steps to 8(lower in order for the model to settle down, then start to increase the grad accum steps)
    # using the same config, initalized model, dataset downloading, model downloading from 2nd phase
    # choosing the 24379393 tokens
    # for each training phase there is a overlap of 1 token at the last needed for the target
    # getting 5952 batches which is perfectly divisible by the 2 processes, each gpu processing 2976 batches
    # warmup steps ratio = 0.15, warmup steps = 111, total steps = 744
    # training with warmup then cosine decay cycle = 1

# using same setting
1st Phase: Tokens up to 24,379,393
2nd Phase: Tokens up to 48,758,786
3rd Phase: Tokens up to 73,138,179
4th Phase: Tokens up to 97,517,569
5th Phase: Tokens up to 121,896,961
6th Phase: Tokens up to 146,276,353
7th Phase: Tokens up to 170,655,747
8th Phase: Tokens up to 195,035,139

# with gradient_accumulation_steps = 32 instead of 8 
9th Phase: Tokens up to 219,414,531

# using eval_dataset to track performance 
10 phase: Tokens upto 243793923, val checking from 1000000 to 2000000
11 phase: Tokens upto 268173315, val checking from 1000000 to 2000000
12 phase: Tokens upto 292552707, val checking from 2000000 to 3000000
13 phase: Tokens upto 316932099, val checking from 3000000 to 4000000
14 phase: Tokens upto 341311491, val checking from 4000000 to 5000000
15 phase: Tokens upto 365690883, val checking dev split to 300000

continued each phase by using subsequent 24,379,393 tokens for training and val split as some random slicing
tried using pad token id for dataset it did not work correctly