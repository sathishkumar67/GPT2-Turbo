# for the first phase of pretraining
    # init model using init method
    # setting grad accum steps to 8(lower in order for the model to settle down, then start to increase the grad accum steps)
    # using the same config, initalized model, dataset downloading, model downloading from 2nd phase
    # choosing the 24379393 tokens
    # for each training phase there is a overlap of 1 token at the last needed for the target
    # getting 5952 batches which is perfectly divisible by the 2 processes, each gpu processing 2976 batches
    # warmup steps ratio = 0.15, warmup steps = 111, total steps = 744
    # training with warmup then cosine decay cycle = 1

# using same setting
    from 1st phase to 8th phase
    
# with gradient_accumulation_steps = 32 instead of 8 
    from 10 phase
    same tokens length
    same schedule

# using eval_dataset to track performance
    from 10th phase
    continued each phase by using subsequent 24,379,393 tokens for training 
    val split as some random slicing
    same schedule

# increasing the no of training tokens from 24379393 to 26738688 from 25th phase
# increasing the no of training tokens from 26738688 to 30146560 from 26th phase

# completed the CC-MAIN-2013-20---000_00000.npy dataset(ran for 29 phases)
# starting next phase training
    using CC-MAIN-2013-20---000_00001.npy dataset(preprocessed the dataset)
    changed the betas parameters from (0.9, 0.97) to (0.9, 0.999)

# from 35th phase 
    increasing the gradient_accumulation_steps from 32 to 128
    changing tokens count from 30146560 to 29360128(which is perfectly divisible by 128)

# from 42nd phase
    changing tokens count from 29360128 to 32505856(which is perfectly divisible by 128)

# from 43rd phase
    changing tokens count from 32505856 to 34603008(which is perfectly divisible by 128)