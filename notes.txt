# for the first phase of pretraining
    # init model using init method
    # setting grad accum steps to 8(lower in order for the model to settle down, then start to increase the grad accum steps)
    # using the same config, initalized model, dataset downloading, model downloading from 2nd phase
    # choosing the 24379393 tokens
    # for each training phase there is a overlap of 1 token at the last needed for the target
    # getting 5952 batches which is perfectly divisible by the 2 processes, each gpu processing 2976 batches
    # warmup steps ratio = 0.15, warmup steps = 111, total steps = 744
    # training with warmup then cosine decay cycle = 1

# using same setting
1st Phase: Tokens up to 24,379,393
2nd Phase: Tokens up to 48,758,786
3rd Phase: Tokens up to 73,138,179
4th Phase: Tokens up to 97,517,569
5th Phase: Tokens up to 121,896,961
6th Phase: Tokens up to 146,276,353
7th Phase: Tokens up to 170,655,747
8th Phase: Tokens up to 195,035,139

with gradient_accumulation_steps = 32 instead of 8 
9th Phase: Tokens up to 219,414,531
# using eval_dataset to track performance 
10 phase: Tokens upto 243793923
11 phase: Tokens upto 268173315