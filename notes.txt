# for the first phase of pretraining
    # init model using init method
    # setting grad accum steps to 8(lower in order for the model to settle down, then start to increase the grad accum steps)
    # using the same config, initalized model, dataset downloading, model downloading from 2nd phase
    # choosing the 24379393 tokens
    # for each training phase there is a overlap of 1 token at the last needed for the target
    # getting 5952 batches which is perfectly divisible by the 2 processes, each gpu processing 2976 batches
    # warmup steps ratio = 0.15, warmup steps = 111, total steps = 744
    # training with warmup then cosine decay cycle = 1

# using same setting
    from 1st phase to 8th phase
    
# with gradient_accumulation_steps = 32 instead of 8 
    from 10 phase
    same tokens length
    same schedule

# using eval_dataset to track performance
    from 10th phase
    continued each phase by using subsequent 24,379,393 tokens for training 
    val split as some random slicing
    same schedule

# increasing the no of training tokens from 24379393 to 26738688 from 25th phase