# need to use custom mask's in attn block for padding tokens(need to check this out)
# need to add extra tokens to check whether the pad tokens are working properly during training, instead of finding the batch_size for gradient_accumulation_steps, it must be dynamic
# need to take samples from the model
# need to download hello swag dataset for eval
# use tqdm for print stats
# use other datasets for pretraining
# develop such it also runs on TPU

1. data allocation
2. data labeling and cleaning
3. hyperparameters strategy
4. scaling law
5. curriculum learning,
6. annealing,
7. training optimization(deepspeed & megatron),
8. training experiments,
9. loss tracking & logging